---
layout: default
title: "故障排查日记（一）"
tags: document
---



# 传统方式的故障排查日记

本文很多命令尽量不要使用到线上，故有运行命令造成gc等问题，本人概不负责。



# 业务简介  

简单来说，业务支撑为java和rabbitMQ，  
java客户端发送消息入到队列 Q-A，进行‘打包’操作，Q-A处理完‘打包’之后，向队列 Q-B 发送一个消息，  
Q-B 中会将打好的包上传CDN。   

Q-A中的 ‘打包’ 操作包括大量IO操作，需要调用系统命令进行大文件MD5获取，zipAlign 字节对其以及包体拷贝等。  
Q-B中主要会占用网络上行进行上传。   
发生问题时，Q-A 并发消费者为10，Q-B 为30.  




# 问题表象  

平日消息堆积不超过2万，某日堆积忽然超过14万。  
有业务激增原因，但同时发现消费能力莫名减弱。
当天尝试重启了一次消费者server，未解决。



# 排查流程  


1. 强行先看看jvm情况

  其实这种情况应该先看CPU占用率，但是为了让本文长一点，先看下JVM。  
  使用ps -aux| grep '服务关键字' 或者 jps 直接定位线程ID， 
  然后，看看内存占用情况，做到心中有数：

  - jmap -heap pid
    我看到New Generation占用了42%，concurrent mark-sweep generation 仅占用 （3%）
    占用并不高，甚至说非常富（lang）裕（fei）

  - jstat -gc pid / jstat -gcutil pid
    验证gc健康度，FGC为5,FGCT为1.887，full gc次数有点高了，毕竟服务刚启动是不是。

  - 看看 full gc 原因，找到gc日志，发现：

   - Heap Inspection Initiated GC

     好吧，我承认我在用了 jmap -histo:live。再看看其他的吧

   - Metadata GC Threshold

     OK，相对我老年代仅仅3%的占用率，这个完全可以优化，但这个是类加载时出现的，并不会影响整体JAVA程序运行效率。  

  到这里我可以基本认为在堆内内存这块是正常的。



2. 看看CPU占用率和锁  

  - top
    可以看到虽然这个JAVA线程赫然排在第一，虽然看%CPU并不是很高，40%左右，但偶尔会干到100%  

  - iostat -c 2

  可以看到 %idle 在50左右徘徊（%iowait   后面说）

  - top -H -p pid
    那就看下是JAVA进程里的具体哪个线程在搞事情吧！

  - printf '%x\n' pid 
    我会使用这个命令将占用最高的 pid 转换为 16 进制得到了 nid 。

  - jstack -l pid

    在jstack 中找到相应的堆栈信息。我更喜欢先把栈信息重定向出来看，或者：

  - jstack pid |grep 'nid' -C5

  其可以直接看到所有线程名称对应的 BLOCKED， TIMED_WAITING 甚至 waiting to lock 等信息。 
  多观察一会，确定了两个nid：

  ```
  "SimpleAsyncTaskExecutor-2" #60 prio=5 os_prio=0 tid=0x00007f854123e800 nid=0x3697 runnable [0x00007f84cc7d2000]
         java.lang.Thread.State: RUNNABLE
              at java.io.FileInputStream.readBytes(Native Method)
              at java.io.FileInputStream.read(FileInputStream.java:255)
              at java.io.BufferedInputStream.read1(BufferedInputStream.java:284)
              at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
              - locked <0x000000068b6f32d8> (a java.lang.UNIXProcess$ProcessPipeInputStream)
              at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
              at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
              at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
              - locked <0x000000068b6f74e8> (a java.io.InputStreamReader)
              at java.io.InputStreamReader.read(InputStreamReader.java:184)
              at java.io.BufferedReader.fill(BufferedReader.java:161)
              at java.io.BufferedReader.readLine(BufferedReader.java:324)
              - locked <0x000000068b6f74e8> (a java.io.InputStreamReader)
              at java.io.BufferedReader.readLine(BufferedReader.java:389)
              at com.baidu.mgame.pack.util.os.OSTools.consumeInputStream(OSTools.java:222)
              at com.baidu.mgame.pack.util.os.OSTools.executeCMD(OSTools.java:79)
              at com.baidu.mgame.pack.util.os.OSTools.zipAlign(OSTools.java:37)
              - locked <0x0000000702093de0> (a [B)
              at com.baidu.pack.service.impl.NetGamePackServiceImpl.pack(NetGamePackServiceImpl.java:134)
              at com.baidu.pack.service.impl.AbstractPackService.ngCommPack(AbstractPackService.java:349)
              at com.baidu.pack.service.impl.AbstractPackService.executePack(AbstractPackService.java:105)
              at com.baidu.pack.task.strategy.PackUpdateCommStrategyImpl.process(PackUpdateCommStrategyImpl.java:36)
              at com.baidu.pack.mq.consumer.DirectConsumerListener.packRequestProcessSync(DirectConsumerListener.java:193)
  
    "SimpleAsyncTaskExecutor-1" #59 prio=5 os_prio=0 tid=0x00007f854123c800 nid=0x3696 waiting for monitor entry [0x00007f84cc853000]
       java.lang.Thread.State: BLOCKED (on object monitor)
    	at com.baidu.mgame.pack.util.os.OSTools.zipAlign(OSTools.java:37)
  
     - waiting to lock <0x0000000702093de0> (a [B)
       at com.baidu.pack.service.impl.NetGamePackServiceImpl.pack(NetGamePackServiceImpl.java:134)
         	at com.baidu.pack.service.impl.AbstractPackService.ngCommPack(AbstractPackService.java:349)
         	at com.baidu.pack.service.impl.AbstractPackService.executePack(AbstractPackService.java:105)
         	at com.baidu.pack.task.strategy.PackUpdateCommStrategyImpl.process(PackUpdateCommStrategyImpl.java:36)
         	at com.baidu.pack.mq.consumer.DirectConsumerListener.packRequestProcessSync(DirectConsumerListener.java:193)
         	at com.baidu.pack.mq.consumer.DirectConsumerListener.processPackUpdate(DirectConsumerListener.java:182)
         	at sun.reflect.GeneratedMethodAccessor141.invoke(Unknown Source)
  ```

  看起来没有死锁，0x00007f854123e800 锁住了 93de0，然后一直在执行read; 

  0x00007f854123c800 在等待 0x00007f854123e800 去释放锁 93de0，而进入 BLOCK

  

  稍等一会，我们再dump一下，发现：

  ```
    "SimpleAsyncTaskExecutor-1" #59 prio=5 os_prio=0 tid=0x00007f854123c800 nid=0x3696 runnable [0x00007f84cc853000]
       java.lang.Thread.State: RUNNABLE
            at java.io.FileOutputStream.writeBytes(Native Method)
            at java.io.FileOutputStream.write(FileOutputStream.java:326)
            at com.baidu.pack.util.FileUtil.copyFile(FileUtil.java:36)
            at com.baidu.pack.service.impl.AbstractPackService.ngCommPack(AbstractPackService.java:324)
            at com.baidu.pack.service.impl.AbstractPackService.executePack(AbstractPackService.java:105)
            at com.baidu.pack.task.strategy.PackUpdateCommStrategyImpl.process(PackUpdateCommStrategyImpl.java:36)
  ```

  果然运行了。




3. well,那看看磁盘IO吧

  - iostat -d 1 10 (或 sar -d 1 10)（推荐iostat）

    ```
    Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
    vda               2.00         4.00         8.00          4          8
    vdb               0.00         0.00         0.00          0          0
    vdc             379.00     39152.00     88244.00      39152      88244
    ```

    kB_read说明有39MB的每秒读取和88MB的每秒写入。

    

- iostat -x -m 1

  ```
  Device:         rrqm/s   wrqm/s     r/s     w/s       rMB/s    wMB/s    avgrq-sz   avgqu-sz   await    svctm  %util
  vda               0.00          2.00           0.00    3.00     0.00        0.02         13.33          0.00             0.33       0.33      0.10
  vdb               0.00          0.00           0.00    0.00     0.00          0.00         0.00          0.00             0.00        0.00      0.00
  vdc             314.00       8250.00    583.00   79.00   102.71    38.88   438.04      154.95       123.65   1.51     100.10
  ```

  看到这里问题已经很明显了，磁盘瓶颈，几个参数作用如下：

   - %util 

     在统计时间内所有处理IO时间，除以总共统计时间。例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的%util = 0.8/1 = 80%，所以该参数暗示了设备的繁忙程度
     。一般地，如果该参数是100%表示设备已经接近满负荷运行了（当然如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。

   - svctm 和 await

     svctm 表示平均每次设备I/O操作的服务时间（以毫秒为单位）。await  表示每一个IO请求的处理的平均时间（单位 ）

     它们的差值越小，则说明队列时间越短，反之差值越大，队列时间越长。如果await的值远高于svctm的值，则表示I/O队列等待太长。

  - avgqu-sz

    平均等待处理的IO请求队列长度

  - avgrq-sz

    平均每次IO操作的数据量(扇区数为单位)

- iostat -c 2

  输出 cpu 使用状况

  ```
  avg-cpu:  %user   %nice %system %iowait  %steal   %idle
                          2.66     0.00       7.73          82.26           0.00    7.35
  ```

  - %iowait

    表示CPU等待IO时间占整个CPU周期的百分比，如果iowait值超过50%，或者明显大于%system、%user以及%idle，表示IO可能存在问题

    



4. 没用上的验证方式

- iotop

  线上不能使用此命令，放弃（我好像暴露了什么）

  iotop 命令来进行定位文件读写的来源。

  不过这边拿到的是 tid，我们要转换成 pid，可以通过 readlink 来找到 pidreadlink -f /proc/*/task/tid/../..。

  找到 pid 之后就可以看这个进程具体的读写情况cat /proc/pid/io

  通过 lsof 命令来确定具体的文件读写情况lsof -p pid

- pidstat -d 1

  这个可以用，意义为展示I/O统计，每秒更新一次。

  在这里我可以看到一些调用系统，而占用IO的命令。

  - strace -o output.txt -T -tt -e trace=all -p pid
    跟踪指定pid进程的所有系统调用（-e trace=all），并统计系统调用的花费时间，以及开始时间（并以可视化的时分秒格式显示），最后将记录结果存在output.txt文件里面。  

  - 通过 pmap 来查看下进程占用的内存情况pmap -x pid | sort -rn -k3 | head -30

  - pstack pid  

    看下系统级别的栈信息，很多重复考虑死锁



那，下一步就根据我们的状况：磁盘性能瓶颈、内存使用率低 来进行优化吧！